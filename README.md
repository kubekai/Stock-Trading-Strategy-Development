# Stock Trading Strategy Development

## 架構

* **資料**：訓練、評估與運行所需的歷史與即時數據。
* **模型**：根據市場輸入產生交易決策。
* **前端**：顯示訊號、績效與操作介面的 UI。
* **後端**：負責邏輯處理、資料運算、推論與整合。

## 資料流程

1. **使用者介面**
   提供策略輸入與回測請求的 Web UI。
2. **資料控制**
   將請求路由至 Flask 後端。
3. **策略模組**
   執行交易策略並產生交易訊號。
4. **回測模組**
   在歷史數據上模擬績效並回報各項指標。

# 實作

金融強化學習（FinRL）是一個用於金融領域深度強化學習（DRL）的開源框架。

## 工作流程

* **資料預處理**：從資料源（Yahoo Finance）載入並前處理數據。
* **環境設計**：自訂與 OpenAI Gym 相容的交易環境。
* **模型訓練**：支援主要 DRL 演算法（A2C、PPO、DDPG、TD3）。
* **回測**：提供報酬、Sharpe 指數與其他績效指標的視覺化結果。

## 資料集

* **股票代號**：2330.TW（台灣半導體製造股份有限公司）
* **資料來源**：Yahoo Finance
* **資料範圍**：2018-01-01 至 2024-12-31
* **特徵**：

| 特徵               | 說明           |
| ---------------- | ------------ |
| 移動平均收斂發散指標（MACD） | 捕捉動能與趨勢追蹤特性。 |
| 相對強弱指標（RSI）      | 測量超買或超賣狀態。   |
| 商品通道指標（CCI）      | 識別週期性趨勢。     |
| 趨向指標（DMI）        | 測量趨勢強度。      |

## 環境參數

* **初始資本**：交易帳戶的起始資金。
* **交易成本**：每次買賣時收取的手續費。
* **最大交易單位（HMAX）**：一次最多可交易的最大股數。
* **狀態空間**：限制每個時間步可交易的股數。
* **獎勵縮放**：將獎勵乘以 0.01 以穩定數值範圍。
* **動盪門檻**：當市場動盪指數超出此值，代理人可能被懲罰或停止交易。

## 績效指標

1. **年化報酬**
   考慮複利後的年平均收益率。
2. **夏普指數**
   單位波動率的超額報酬，衡量風險調整後績效。
3. **最大回撤**
   投資組合從最高點到最低點的最大跌幅。

# 模型選擇

## 演算法比較

| 演算法      | 年化報酬   | 最大回撤   |
| -------- | ------ | ------ |
| **A2C**  | 約 7.8% | 約 5.3% |
| **PPO**  | 約 4.8% | 約 3.3% |
| **DDPG** | 約 7.8% | 約 5.3% |
| **TD3**  | 約 7.8% | 約 5.3% |

## 優缺點

* **A2C（Advantage Actor-Critic）**

  * 架構簡單、計算成本低。
  * 對金融數據噪聲較為敏感。
* **PPO（Proximal Policy Optimization）**

  * 訓練穩定，避免崩潰。
  * 收益穩定但可能錯失大幅波動。
* **DDPG（Deep Deterministic Policy Gradient）**

  * 精細控制持倉量。
  * 回測時可能陷入區域最優並劇烈波動。
* **TD3（Twin Delayed DDPG）**

  * 相較 DDPG 更加穩定。
  * 架構較複雜。

## 選擇理由

* 兼顧收益與風險的較佳平衡。
* 演算法彈性高、易於擴充。
* 實作門檻低、計算成本小。
* 回測表現穩定、策略易於理解。

# 結論

* **演算法**：選擇 A2C 作為主要訓練演算法。
* **應用**：開發單一股票回測平台，允許使用者輸入資料進行回測。
* **未來規劃**：擴展系統以支援多檔股票投組回測。
